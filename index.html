<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Deep South Logic</title>
  <style>
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      background: #0f0f0f;
      color: #d6d6d6;
      font-family: 'Courier New', Courier, monospace;
      padding: 2rem;
      line-height: 1.6;
    }

    h1 {
      font-size: 2.5rem;
      margin-bottom: 1rem;
      color: #c2ff00;
      animation: glitch 1s infinite linear alternate-reverse;
    }

    h2 {
      margin-top: 2rem;
      border-bottom: 1px solid #444;
      padding-bottom: 0.5rem;
      color: #999;
    }

    p {
      margin: 1rem 0;
    }

    .glitch {
      position: relative;
      display: inline-block;
    }

    .glitch::before,
    .glitch::after {
      content: attr(data-text);
      position: absolute;
      left: 0;
      width: 100%;
      overflow: hidden;
      color: #f0f;
      clip: rect(0, 900px, 0, 0);
    }

    .glitch::before {
      animation: glitchTop 2s infinite linear alternate-reverse;
    }

    .glitch::after {
      color: #0ff;
      animation: glitchBottom 2s infinite linear alternate-reverse;
    }

    @keyframes glitchTop {
      0% { clip: rect(0, 9999px, 0, 0); }
      10% { clip: rect(10px, 9999px, 40px, 0); }
      20% { clip: rect(85px, 9999px, 140px, 0); }
      100% { clip: rect(0, 9999px, 0, 0); }
    }

    @keyframes glitchBottom {
      0% { clip: rect(0, 9999px, 0, 0); }
      10% { clip: rect(45px, 9999px, 80px, 0); }
      20% { clip: rect(100px, 9999px, 130px, 0); }
      100% { clip: rect(0, 9999px, 0, 0); }
    }

    footer {
      margin-top: 3rem;
      font-size: 0.8rem;
      color: #555;
      text-align: center;
    }
  </style>
</head>
<body>
  <h1 class="glitch" data-text="Deep South Logic">Deep South Logic</h1>

  <section>
    <h2>Who am I?</h2>
    <p>
      Just another node in a neural net called consciousness. I write code. I train models. I stare into loss functions long enough to see my soul overfit.
    </p>
  </section>

  <section>
    <h2>Deep Learning Rants</h2>
    <p>"Time is a flat loss curve. Every epoch just a new loop, no final convergence... just entropy."</p>
    <p>"I stare into the gradients, and they stare back. We optimize, not to learn — but to decay."</p>
    <p>"In the end, the weights mean nothing. Just illusions projected on a multidimensional manifold."</p>
    <p style="color: #888;">#MachineLearning #NeuralNihilism #RustCohleVibes</p>
  </section>

  <section>
    <h2>Current Status</h2>
    <p>
      Training: unsupervised<br>
      Activation: leaky<br>
      Regularization: none<br>
      Reality: overfitting
    </p>
  </section>

  <footer>
    &copy; 2025 DeepSouthLogic — No gradients were harmed in the making of this site.
  </footer>
</body>
</html>
